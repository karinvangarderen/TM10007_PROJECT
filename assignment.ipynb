{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment ADNI Dataset\n",
        "Bram Gerritse 4462599  \n",
        "Saskia Bijl 4439457  \n",
        "Karan Ramsodit  \n",
        "Enzo Kerkhof 44488555"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiDn2Sk-VWqE",
        "outputId": "33087709-02fc-4083-962e-637556800de5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run this to use from colab environment to install git repo\n",
        "!pip install -q --upgrade git+https://github.com/Enzo-Kerkhof/TM10007_PROJECT.git@random_forest\n",
        "\n",
        "# Use the @ to determine what branch to install. Don't use to install master."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for brats (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gWpMLRN2tcyM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f64cfdc7-a7b1-4995-d8dd-a56699fb2ef3"
      },
      "source": [
        "# Import used libraries\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn import decomposition\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_VumKdpfpKGc"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-NE_fTbKGe5z",
        "outputId": "0bfcf026-9c3e-4c84-f795-3bae2025fe46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Data loading functions. Uncomment the one you want to use\n",
        "from adni.load_data import load_data\n",
        "\n",
        "data = load_data()\n",
        "print('The dataset ABNI contains the following\\n')\n",
        "print(f'The number of samples/patients: {len(data.index)}')\n",
        "print(f'The number of columns/features: {len(data.columns)}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset ABNI contains the following\n",
            "\n",
            "The number of samples/patients: 855\n",
            "The number of columns/features: 268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3SYPh4rAibQ",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEkOuwv8AlwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop labels from dataframe\n",
        "X = data.drop(['label'],axis=1)\n",
        "\n",
        "# Binerize labels AD = 1, CN = 0\n",
        "Y = preprocessing.label_binarize(data['label'], ['CN', 'AD'])\n",
        "Y = [i[0] for i in Y]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.8)\n",
        "y_train = np.array(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OKNDWlmAudL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale the data to be normal\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u-1zomrAxdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform a PCA\n",
        "pca = decomposition.PCA(n_components=100)\n",
        "pca.fit(X_train_scaled)\n",
        "X_train_pca = pca.transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muepMe-dA0Ac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "50624cf1-5538-4ab2-82af-6566a2206ee4"
      },
      "source": [
        "# Now first use the selectfrom model module. Select all features with a weight above the median.\n",
        "selector = feature_selection.SelectFromModel(estimator=Lasso(alpha=10**(-10)), threshold='median')\n",
        "selector.fit(X_train_scaled, y_train)\n",
        "n_original = X_train_scaled.shape[1]\n",
        "X_train_selected = selector.transform(X_train_scaled)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "n_selected = X_train_selected.shape[1]\n",
        "print(f\"Selected {n_selected} from {n_original} features.\")\n",
        "\n",
        "\n",
        "X_train = X_train_selected\n",
        "X_test = X_test_selected"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected 134 from 267 features.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0053341334447171195, tolerance: 0.003923976608187134\n",
            "  positive)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyPKA6LcA8hx",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7YqIZxUA20O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us first see how much of a difference the number of trees, n_estimators, makes\n",
        "n_trees = [50]\n",
        "    \n",
        "# Now use the classifiers on all datasets\n",
        "for n_tree in n_trees:\n",
        "  for X, Y in zip(X_train, y_train):\n",
        "    clf = RandomForestClassifier(n_estimators=n_tree)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdYvAlr1BEx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next, let us try to see how bootstrapping influences the process.\n",
        "# Let us first see how much of a difference the number of trees, n_estimators, makes\n",
        "clsfs = [RandomForestClassifier(n_estimators=50, bootstrap=False)]\n",
        "    \n",
        "# Now use the classifiers on all datasets\n",
        "for clf in clsfs:\n",
        "    for X, Y in zip(X_train, y_train):\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8GXzi9gBJjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lastly, if you have an imbalance in your dataset, or one class is more important than the other, you may want\n",
        "# to alter the class weigh in the random forest.\n",
        "clsfs = [RandomForestClassifier(class_weight={0: 1, 1: 0.001}),\n",
        "         RandomForestClassifier(class_weight={0: 1, 1: 1}),\n",
        "         RandomForestClassifier(class_weight={0: 1, 1: 10}),\n",
        "         RandomForestClassifier(class_weight={0: 1, 1: 100})]\n",
        "    \n",
        "# Now use the classifiers on all datasets\n",
        "for clf in clsfs:\n",
        "    for X, Y in zip(X_train, y_train):\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}